\documentclass[a4paper, 11pt, openany]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,top=1.5cm,bottom=1.5cm,left=2cm,right=2cm]{geometry}
\usepackage{tikz}

\usepackage[caption=false]{subfig}
\usepackage[noend]{algpseudocode}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}
\theoremstyle{plain}% Theorem-like structures
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

%\usepackage{apacite}
%\usepackage[round]{natbib}
\usepackage[shortlabels]{enumitem}

\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\argmin}{\arg\min}

\usepackage{resizegather}



\begin{document}
	\title{Selecting the stepsizes in Polyak's Momentum algorithm by quadratic planar search}
	
	\author{
		\textbf{Luigi Grippo, Matteo Lapucci, Sefano Lucidi, Marco Sciandrone}
	}
	\date{DINFO, Universit\`{a} degli Studi di Firenze,\\DIAG, Sapienza Università di Roma}
	\maketitle
	\thispagestyle{empty}
	
%	\begin{abstract}
%		Tu ne quaesieris, scire nefas, quem mihi quem tibi finem Di dederint, Leucone, nec Babylonios temptaris numeros. Ut melius, quidquid erit pati! Seu pluris hiemes, seu tribuit Iuppiter ultimam, quae nunc oppositis debilitat pumicibus mare Tyrrhenum, sapias: vina liques et spatio brevi spem longam reseces. Dum loquimur fugerit invida aetas. Carpe diem, quam minimum quareula postero. 
%	\end{abstract}
%	
	%\input{files/introduction}
	%\input{files/preliminaries}
	%\input{files/smo}
	%\input{files/convegence}
	
	\section{Momentum with Inexact Plane Search: A Summary}
	The classical gradient descent (GD) algorithm is defined by the iteration
	$$x^{k+1} = x^k-\alpha_k \nabla f(x^k),$$
	where $\alpha_k$ is a suitable stepsize usually computed performing a (inexact) line search (e.g., Armijo or Wolfe methods \cite{bertsekas1997nonlinear}). More general first-order optimizers carry out iterations of the form
	\begin{equation}
		\label{eq:first_order_update}
		x^{k+1} = x^k-\alpha_k H_k\nabla f(x^k),
	\end{equation}
	where $H_k$ is a positive definite matrix; of course selecting $H_k=I$ leads back to  gradient descent, whereas $H_k \approx (\nabla^2f(x^k))^{-1}$ gives rise to Quasi-Newton methods \cite{bertsekas1997nonlinear}. \textcolor{red}{[Add longer discussion on first-order method]}. 
	
	The idea of introducing $H_k$ is to exploit curvature information to improve the quality of the search direction.
	
	A different strategy to speed up the computation was firstly proposed by Polyak \cite{polyak1964some}; the idea of Polyak consists of exploiting information about past iterates to speed up the process; in particular, the iterations of Polyak Heavy-ball or Momentum method have the form:
	\begin{equation}
		\label{eq:momentum_update}
	\end{equation}
	$$x^{k+1} = x^k -\alpha_k \nabla f(x^k) + \beta (x^{k}-x^{k-1}).$$
	The idea of the above update is that the direction of last iteration will arguably be a good descent direction even at the current one. Partly repeating the previous step helps with controlling oscillation and providing acceleration in low curvature regions.
	
	The main computational advantage of introducing the \textit{momentum} term lies in the fact that it only exploits already computed information: no additional evaluations of the objective function or, even worse, the gradient, are required.
	
	In the case of strictly convex quadratic functions, the ``optimal'' values \textcolor{red}{[argomentare meglio su cosa si intende per ottimali]} can be determined, a priori, as follows \cite{polyak1987introduction}:
	$$\alpha^\star = \frac{4}{(\sqrt{\lambda_M} + \sqrt{\lambda_m})^2},\qquad \beta^\star = \left(\frac{\sqrt{\lambda_M} - \sqrt{\lambda_m})}{\sqrt{\lambda_M} + \sqrt{\lambda_m}}\right)^2$$
	where $\lambda_M$ and $\lambda_m$ are the largest and smallest eigenvalues of the Hessian matrix, respectively. The choice of this optimal parameters allows to obtain a local accelerated linear	convergence rate with iteration error $\mathcal{O}((\frac{\sqrt{\lambda_M} - \sqrt{\lambda_m}}{\sqrt{\lambda_M} + \sqrt{\lambda_m}})^k)$. \textcolor{red}{[Inserire qui commento su equivalenza con gradiente coniugato; io non sono riuscito a trovare riferimenti]}
	
	The (local) linear convergence rate (with better factor than both GD and Nesterov's method \textcolor{red}{[parlare brevemente di nesterov da qualche parte]}), can be generalized under hypotheses of twice continuous differentiability, Lipschitz-continuous gradient and strong convexity \cite{polyak1964some}.  
	
	Sufficient conditions for the heavy-ball method to converge to critical points in the nonconvex setting have been proven by \cite{zavriev1993heavy}. However, counterexamples have been recently provided \cite{lessard2016analysis} to show that the heavy-ball method does not necessarily converge a) in the convex case under twice differentiability assumptions and b) in the strongly convex (but not twice differentiable) case, even if one chooses Polyak's optimal step-size parameters. 
	As of today, no evidence has been found that Polyak's method outperforms Nesterov’s method, or even the basic GD, when the objective is not twice continuously differentiable. 
	
	Under convexity and smoothness assumptions,
	the ergodic $\mathcal{O}(1/k)$, i.e., $f(\sum_{i=1}^{k}\frac{x^i}{k}) - \min f = \mathcal{O}(1/k)$, has been proved \cite{ghadimi2015global}. In the same work, the algorithm is proved to be globally convergent with linear speed in the strongly convex case (even though for $\beta$ in a possibly small range of values).
	
	Due to the inertial term $\beta(x^{k} - x^{k-1})$, momentum, in contrast to gradient descent, does not satisfy Fejér monotonicity \cite{Combettes2009}. Hence, it is difficult to prove its non-ergodic convergence rates. To this end, a novel Lyapunov analysis was designed and better convergence results were derived \cite{sun2019non}, like the non-ergodic sublinear convergence rate $f(x^k) - \min f = \mathcal{O}(1/k)$ and larger step size for linear convergence.
	
	Except for the quadratic case, the selection of parameters $\alpha$ and $\beta$ is not trivial; $\alpha$ might be chosen by a line search, whereas $\beta$ is usually set to a constant chosen somewhat heuristically; some adaptive strategies have also been proposed in recent years \textcolor{red}{[approfondire un po' questa cosa, serve per motivare in modo più robusto la nostra idea]}
		
	The proposal of this paper consists in a novel approach for the selection of the two parameters $\alpha$ and $\beta$ in \eqref{eq:momentum_update}. Similarly as what is commonly done for the selection of the stepsize $\alpha$ in algorithms of the form \eqref{eq:first_order_update}, here we try to (approximately) solve the optimization problem in two variables
	$$\min_{\alpha,\beta}\;\phi(\alpha,\beta) = f(x^k -\alpha \nabla f(x^k) + \beta (x^{k}-x^{k-1})),$$
	or, more generally,
	$$\min_{\alpha,\beta}\;\phi(\alpha,\beta) = f(x^k +\alpha d_1 + \beta d_2).$$
	Computing the objective function and the gradient for this subproblem unfortunately requires to compute the corresponding function value and gradients of the original function of $n$ variables at points we have no information about:
	$$\phi_k(\alpha,\beta) = f(x^k +\alpha d_1 + \beta d_2)$$
	$$\nabla \phi_k(\alpha,\beta) = (\nabla f(x^k +\alpha d_1 + \beta d_2)^Td_1,\nabla f(x^k +\alpha d_1 + \beta d_2)^Td_2)^T= D\nabla f(x^k +\alpha d_1 + \beta d_2),$$
	where $D=(d_1,d_2)^T$.
	For the sake of completeness, let us remark that
	$$\nabla^2\phi_k(\alpha,\beta) = D\nabla^2f(x^k +\alpha d_1 + \beta d_2)D^T.$$
	
	Selecting the step sizes solving, even inexactly, the above subproblem, we can reasonably expect to reduce the number of iterations of algorithm REF \textcolor{red}{[this claim is supported by computational experiments]}; however, at each iteration of the solver employed to tackle the subproblem, $f$ and $\nabla f$ of the original problem will be computed several times, leading to an overall increase in the computational cost of the algorithm \textcolor{red}{[this claim is also supported by numerical evidence]}.
 	
	Ideally, we would like the subproblem to be an actual two-variables problem, for which computing the function value or the gradient is cheap. Our idea is to exploit Taylor's approximation to construct a surrogate function to optimize in the subproblem. \textcolor{red}{[dire che l'approssimazione al primo ordine dava risultati schifosi...]}
	
	The second order expansion of $\phi_k(\alpha,\beta)$ around $(0,0)$ is given by:
	$$\phi_k(\alpha,\beta)\approx \phi_k(0,0) + \nabla \phi_k(0,0)^T(\alpha,\beta) + \frac{1}{2}(\alpha,\beta)^T\nabla^2\phi_k(0,0)(\alpha,\beta),$$
	i.e.,
	$$\phi_k(\alpha,\beta)\approx f(x^k) +  (D \nabla f(x^k))^T(\alpha,\beta) + \frac{1}{2}(\alpha,\beta)^T D \nabla^2f(x^k) D^T(\alpha,\beta).$$
	
	Now, at iteration $k$ we clearly compute $f(x^k)$ and $\nabla f(x^k)$, so the constant and the linear terms in the above approximation are basically known; on the other hand, we obviously do not want to compute the Hessian matrix at $x^k$, thus we do not have access to the exact values of $\nabla^2\phi_k(0,0)$. However, we can estimate the three degrees of freedom for the (symmetric) quadratic part of the approximation by interpolation \textcolor{red}{[strategia alla Powell?]}. Of course, interpolation at $(\alpha,\beta) = (0,0)$ does not provide information, as we only get the trivial equality $f(x^k) = f(x^k)$. We can exploit the information from iteration $k-1$, as we clearly know $f(x^{k-1})$ and $\nabla f(x^{k-1})$: $x^{k-1}$ indeed lies on the plane generated by $d_1=-\nabla f(x^k)$ and $d_2 = (x^k-x^{k-1})$ passing through $x^k$, specifically obtainable for $(\alpha,\beta) = (0,-1)$. 
	Thus, in order to define the quadratic subproblem in two variables, we only need to compute the objective value at two additional points, which is a reasonable cost, comparable to that of a common line-search. 
	
	Two reasonable candidate points to evaluate the function at might be, for example, $(\alpha_{k-1},\beta_{k-1})$ and $(\alpha_{k-1},0)$. The interpolation system to obtain the quadratic matrix $B$,
	\begin{gather}
		\label{eq:lin_sys}
		\frac{1}{2}\left(\begin{array}
			{ccc}
			0&0&1\\\alpha_{k-1}^2&0&0\\\alpha_{k-1}^2&2\alpha_{k-1}\beta_{k-1}&\beta_{k-1}^2
		\end{array}\right)\left(\begin{array}{c}
			B_{11}\\B_{12}\\B_{22}
		\end{array}\right) = \left(\begin{array}{c}
			f(x^{k-1})-f(x^k)+\nabla_\beta \phi_k(0,0)\\
			f(x^k+\alpha_{k-1}d_1) - f(x^k) - \alpha_{k-1}\nabla_\alpha \phi_k(0,0)\\
			f(x^k+\alpha_{k-1}d_1 + \beta_{k-1}d_2) - f(x^k) - \alpha_{k-1}\nabla_\alpha \phi_k(0,0) - \beta_{k-1}\nabla_\beta \phi_k(0,0)
		\end{array}\right),
	\end{gather}
	can be easily solved in closed form:
	\begin{gather*}
		B_{11} = \frac{2}{\alpha_{k-1}^2}(f(x^k+\alpha_{k-1}d_1)-\alpha_{k-1}\nabla_\alpha \phi_k(0,0)-f(x^k))\\ B_{22} = 2(\nabla_\beta \phi_k(0,0)+f(x^{k-1})-f(x^k))\\ B_{12} = B_{21} = \frac{f(x^k+\alpha_{k-1}d_1+\beta_{k-1}d_2)-f(x^k)-\alpha_{k-1}\nabla_\alpha \phi_k(0,0)-\beta_{k-1}\nabla_\beta \phi_k(0,0)-0.5\alpha_{k-1}^2B_{11}-0.5\beta_{k-1}^2B_{22}}{\alpha_{k-1}\beta_{k-1}}
	\end{gather*}
	
	Then, $\alpha_{k}$ and $\beta_k$ can be obtained solving the simple linear system in two variables $$B(\alpha,\beta) = -\nabla \phi(0,0).$$
	
	The overall resulting algorithm is reported here below
	
	\begin{algorithm}[htbp]
			\SetAlCapHSkip{0.22cm}
			\caption{\texttt{Momentum with Plane Search}}
			\SetInd{0.1cm}{0.5cm}
			\SetVlineSkip{0.1cm}
			\SetNlSkip{0.2cm}
			
			
			\label{alg:PGLS}
			Input: $x^0,x^{-1}$, $\alpha_{-1}$, $\beta_{-1}$.\\
			Set $k=0$\\	
			\While{$\|\nabla f(x^k)\|\le \epsilon$}{Set $d_1^k = -\nabla f(x^k)$, $d_2^k = (x^k-x^{k-1})$, $D_k = (d_1^k, d_2^k)^T$\\Compute $\nabla\phi_k(0,0) = D_k\nabla f(x^k)$\\ Compute $B_k$ solving system \eqref{eq:lin_sys}\\
			Compute $\hat{\alpha}_k, \hat{\beta}_k$ solving $B(\alpha,\beta) = -\nabla\phi_k(0,0)$\\
			Set $\hat{x}^{k+1} = x^k+\hat{\alpha}_kd_1 + \hat{\beta}_kd_2$\\
			\If{$f(\hat{x}^{k+1})< f(x^k)$}{
				Set $\alpha_{k}, \beta_k = \hat{\alpha}_k, \hat{\beta}_k$\\
				Set $x^{k+1} = \hat{x}^{k+1}$
			}
			\Else{
			Set $\beta_k=0$\\ Compute $\alpha_k$ by an Armijo line-search (with initial stepsize defined according to Barzilai-Borwein) \\Set $x^{k+1}=  x^k+\alpha_kd_1$
			} 
			Set $k=k+1$
			} 
			\Return $x^k$
	\end{algorithm}


Additional elements of the algorithm to be described:
\begin{itemize}
	\item After unsuccessful iterations where the plane search does not lead to an objective decrease, a number of consecutive steps of Barziali-Borwein should be performed to fully exploit the power of this version of gradient descent; 
	\item If either $\alpha_{k-1}$ or $\beta_{k-1}$ is equal to zero, the linear system for the computation of $B$ is not full-rank; in this cases, use $\alpha_\delta= \delta >0$ or $\beta_\delta = \delta >0$ to define the two interpolation points;
	\item Moreover, if either $|\alpha_{k-1}|<\varepsilon$ or $|\beta_{k-1}|<\varepsilon$, use $\alpha_\delta= \delta\text{sign}(\alpha_{k-1})$ or $\beta_\delta = \delta\text{sign}(\beta_{k-1})$ to define the two interpolation points, otherwise the quadratic parameters of the approximation might result in being ``too local'';
	\item the linear system $B(\alpha,\beta) = -\nabla f(0,0)$ might be not full-rank; in that case, solve it as by regularized least-squares;
	\item If the obtained values of $\alpha_k$ and $\beta_k$ do not provide a decrease in the objective function, they can be used to recompute the quadratic approximation matrix $B$ with higher accuracy; since we have computed $f(x^k+\alpha_kd_1+\beta_kd2)$, we can add a row in the linear system, which can then be solved again by least squares; this strategy could be used repeatedly up to a predefined number of times (e.g., 3).
\end{itemize}	

	
	
	\section{Experiments}
	\subsection{A}
	Momentum constant stepsize vs. derivative-free plane search vs. plane search vs. plane search multistart vs. quadratic plane search
	\subsection{B}
	momentum-QPS vs. QPS with random second direction
	\subsection{C}
	momentum-qps vs. scipy-cg vs. scipy-lbfgs
	
	
	 
	
	
	\bibliographystyle{abbrv}
	\bibliography{bibliography}
	

\end{document}